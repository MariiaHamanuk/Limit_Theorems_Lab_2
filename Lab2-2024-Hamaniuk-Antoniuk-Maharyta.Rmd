---
title: 'P&S-2025 Lab assignment 2'
author: "Hamaniuk, Antoniuk, Maharyta"
output:
  html_document:
    df_print: paged
---

## General comments and instructions

-   Complete solution will give you **4 points** (working code with explanations + oral defense). Submission deadline **November 1, 2023, 22:00**\
-   The report must be prepared as an *R notebook*; you must submit to **cms** both the source *R notebook* **and** the generated html file\
-   At the beginning of the notebook, provide a work-breakdown structure estimating efforts of each team member\
-   For each task, include
    -   problem formulation and discussion (what is a reasonable answer to discuss);\
    -   the corresponding $\mathbf{R}$ code with comments (usually it is just a couple of lines long);\
    -   the statistics obtained (like sample mean or anything else you use to complete the task) as well as histograms etc to illustrate your findings;\
    -   justification of your solution (e.g. refer to the corresponding theorems from probability theory);\
    -   conclusions (e.g. how reliable your answer is, does it agree with common sense expectations etc)\
-   The **team id number** referred to in tasks is the **two-digit** ordinal number of your team on the list. Include the line **set.seed(team id number)** at the beginning of your code to make your calculations reproducible. Also observe that the answers **do** depend on this number!\
-   Take into account that not complying with these instructions may result in point deduction regardless of whether or not your implementation is correct.

------------------------------------------------------------------------

### Task 1

#### In this task, we discuss the $[7,4]$ Hamming code and investigate its reliability. That coding system can correct single errors in the transmission of $4$-bit messages and proceeds as follows:

-   given a message $\mathbf{m} = (a_1 a_2 a_3 a_4)$, we first encode it to a $7$-bit *codeword* $\mathbf{c} = \mathbf{m}G = (x_1 x_2 x_3 x_4 x_5 x_6 x_7)$, where $G$ is a $4\times 7$ *generator* matrix\
-   the codeword $\mathbf{c}$ is transmitted, and $\mathbf{r}$ is the received message\
-   $\mathbf{r}$ is checked for errors by calculating the *syndrome vector* $\mathbf{z} := \mathbf{r} H$, for a $7 \times 3$ *parity-check* matrix $H$\
-   if a single error has occurred in $\mathbf{r}$, then the binary $\mathbf{z} = (z_1 z_2 z_3)$ identifies the wrong bit no. $z_1 + 2 z_2 + 4z_3$; thus $(0 0 0)$ shows there was no error (or more than one), while $(1 1 0 )$ means the third bit (or more than one) got corrupted\
-   if the error was identified, then we flip the corresponding bit in $\mathbf{r}$ to get the corrected $\mathbf{r}^* = (r_1 r_2 r_3 r_4 r_5 r_6 r_7)$;\
-   the decoded message is then $\mathbf{m}^*:= (r_3r_5r_6r_7)$.

#### The **generator** matrix $G$ and the **parity-check** matrix $H$ are given by

$$  
    G := 
    \begin{pmatrix}
        1 & 1 & 1 & 0 & 0 & 0 & 0 \\
        1 & 0 & 0 & 1 & 1 & 0 & 0 \\
        0 & 1 & 0 & 1 & 0 & 1 & 0 \\
        1 & 1 & 0 & 1 & 0 & 0 & 1 \\
    \end{pmatrix},
 \qquad 
    H^\top := \begin{pmatrix}
        1 & 0 & 1 & 0 & 1 & 0 & 1 \\
        0 & 1 & 1 & 0 & 0 & 1 & 1 \\
        0 & 0 & 0 & 1 & 1 & 1 & 1
    \end{pmatrix}
$$

#### Assume that each bit in the transmission $\mathbf{c} \mapsto \mathbf{r}$ gets corrupted independently of the others with probability $p = \mathtt{id}/100$, where $\mathtt{id}$ is your team number. Your task is the following one.

1.  Simulate the encoding-transmission-decoding process $N$ times and find the estimate $\hat p$ of the probability $p^*$ of correct transmission of a single message $\mathbf{m}$. Comment why, for large $N$, $\hat p$ is expected to be close to $p^*$.\
2.  By estimating the standard deviation of the corresponding indicator of success by the standard error of your sample and using the CLT, predict the \emph{confidence} interval $(p^*-\varepsilon, p^* + \varepsilon)$, in which the estimate $\hat p$ falls with probability at least $0.95$.\
3.  What choice of $N$ guarantees that $\varepsilon \le 0.03$?\
4.  Draw the histogram of the number $k = 0,1,2,3,4$ of errors while transmitting a $4$-digit binary message. Do you think it is one of the known distributions?

**1**

#### First, we set the **id** of the team and define the probability $p$ and the generator and parity-check matrices $G$ and $H$

```{r}
# team id number 
id <- 35                 

set.seed(id)
p <- id/100 

# matrices G and H
G <- matrix(c(1, 1, 1, 0, 0, 0, 0,
		1, 0, 0, 1, 1, 0, 0,
		0, 1, 0, 1, 0, 1, 0,
		1, 1, 0, 1, 0, 0, 1), nrow = 4, byrow = TRUE)
H <- t(matrix(c(1, 0, 1, 0, 1, 0, 1,
		0, 1, 1, 0, 0, 1, 1,
		0, 0, 0, 1, 1, 1, 1), nrow = 3, byrow = TRUE))

mod2 <- function(x) x %% 2

#cat("The matrix G is: \n") 
#G  
#cat("The matrix H is: \n") 
#H
#cat("The product GH must be zero: \n")
#mod2(G%*%H)
```

#### Next, we generate the messages

```{r}
# generate N messages

message_generator <- function(N) {
  matrix(sample(c(0,1), 4*N, replace = TRUE), nrow = N)
}  

N_mess = 100
messages <- message_generator(N_mess)
codewords <- mod2(messages %*% G)

```

#### Generate random errors; do not forget that they occur with probability $p$! Next, generate the received messages

```{r}

E <- matrix(rbinom(N_mess*7, 1, p), nrow = N_mess, ncol = 7) # noice
r <- mod2(codewords + E) # what we resieve
```

**Detecting errors**

```{r}
Z <- mod2(r %*% H) # check which bits are corrupted
j <- Z[,1] + 2*Z[,2] + 4*Z[,3] # error index
```

**Correcting them**

```{r}
correct <- matrix(0L, nrow = N_mess, ncol = 7)
idx <- which(j>0)
if (length(idx)>0) correct[cbind(idx, j[idx])] <- 1L
r_fixed <- mod2(r + correct)
```

**Decoding**

```{r}
m_dec <- r_fixed[, c(3, 5, 6, 7), drop=FALSE]
success <- rowSums(m_dec != messages) == 0
p_hat <- mean(success)
res <- list(phat=p_hat, success=success,
       codewords=codewords, received=r, received_fix=r_fixed, Z=Z, j=j)
```

**Now let's check how close we are**

```{r}
p_theory <- (1-p)^7 + 7*p*(1-p)^6 #p*

cat(sprintf("Emperical p*: %.5f\n", p_hat))
cat(sprintf("Theoretical p*: %.5f\n\n", p_theory))
```

We expect $\hat p$ to be close to $p^*$ for large $N$ because of the Weak Law of Large Numbers.

**2**

For estimated standard error we have this formula:

$$\widehat{SE} = \sqrt{\frac{\hat p(1-\hat p)}{N}}$$

Let's find it.

```{r}
SE <- sqrt(p_hat*(1-p_hat)/N_mess)
SE
```

Now we can find **approximate** 95% confidence interval for the true success probability $p^*$

​$$\hat p \pm 1.96\widehat{SE}$$

```{r}
eps <- 1.96*SE
cat(sprintf("95%% CI: [%.5f, %.5f]  (ε = %.5f)\n\n", p_hat - eps, p_hat + eps, eps))
```

**3**

Here we need:

$$
1.96\sqrt{\frac{\hat p(1-\hat p)}{N}} \leq 0.03
$$

The worst case for Var of Binomial distribution is $\hat p(1-\hat p) \leq \frac{1}{4}$.

So:

$$
N \geq (\frac{1.96\sqrt{0.25}}{0.03})^2
$$

```{r}
N_needed <- ceiling( (1.96*sqrt(0.25)/0.03)^2 )
cat(sprintf("Minimum N for ε ≤ 0.03: %d\n\n", N_needed))
```

**4**

Now let's see the histogram.

```{r}
M  <- 20000
k <- rbinom(M, size = 4, prob = p)

tab <- table(factor(k, levels=0:4))
freq <- as.numeric(tab) / sum(tab)

kvals <- 0:4
probs <- dbinom(kvals, size = 4, prob = p)

op <- par(mar=c(4,4,2,1))
bp <- barplot(freq,
              names.arg = kvals,
              xlab = "k (num of errors)",
              ylab = "probability",
              main = sprintf("Errors in 4-bit transmission (p=%.2f)", p))

points(bp, probs, pch = 19) # black - theoretical
lines(bp, probs, lwd = 2)
legend("topright", c("Emperical", "Theor. Bin(4,p)"),
       pch = c(15,19), lty = c(0,1), bty = "n")
par(op)


```

And that's Binomial distribution $k \sim Bin(4,p)$!

### Little summary

We learned how reliable the Hamming [7,4] code is under bit-flip noise p=id/100: by simulation we get $\hat p$​, the fraction of messages that decode correctly, which converges to the true success probability $p^*$. We quantify uncertainty with a 95% confidence interval $\hat p \pm 1.96\sqrt{\hat p(1-\hat p)/N}$​, so more trials make the estimate tighter. To guarantee a margin of at most 0.03, about $N≥1068$ runs are needed. And the number of errors when sending an uncoded 4-bit message follows a Binomial(4,p) distribution. That's it.

------------------------------------------------------------------------

### Task 2.

In this task, we discuss a real-life process that is well modelled by a Poisson distribution. As you remember, a Poisson random variable describes occurrences of rare events, i.e., counts the number of successes in a large number of independent random experiments. One of the typical examples is the **radioactive decay** process.

Consider a sample of radioactive element of mass $m$, which has a big *half-life period* $T$; it is vitally important to know the probability that during a one second period, the number of nuclei decays will not exceed some critical level $k$. This probability can easily be estimated using the fact that, given the *activity* ${\lambda}$ of the element (i.e., the probability that exactly one nucleus decays in one second) and the number $N$ of atoms in the sample, the random number of decays within a second is well modelled by Poisson distribution with parameter $\mu:=N\lambda$. Next, for the sample of mass $m$, the number of atoms is $N = \frac{m}{M} N_A$, where $N_A = 6 \times 10^{23}$ is the Avogadro constant, and $M$ is the molar (atomic) mass of the element. The activity of the element, $\lambda$, is $\log(2)/T$, where $T$ is measured in seconds.

Assume that a medical laboratory receives $n$ samples of radioactive element ${{}^{137}}\mathtt{Cs}$ (used in radiotherapy) with half-life period $T = 30.1$ years and mass $m = \mathtt{team\, id \,number} \times 10^{-6}$ g each. Denote by $X_1,X_2,\dots,X_n$ the **i.i.d. r.v.**'s counting the number of decays in sample $i$ in one second.

1.  Specify the parameter of the Poisson distribution of $X_i$ (you'll need the atomic mass of *Cesium-137*)\
2.  Show that the distribution of the sample means of $X_1,\dots,X_n$ gets very close to a normal one as $n$ becomes large and identify that normal distribution. To this end,
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and form the empirical cumulative distribution function $\hat F_{\mathbf{s}}$ of $\mathbf{s}$;
    -   identify $\mu$ and $\sigma^2$ such that the \textbf{c.d.f.} $F$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the \textbf{e.c.d.f.} $\hat F_{\mathbf{s}}$ and plot both **c.d.f.**'s on one graph to visualize their proximity (use the proper scales!);
    -   calculate the maximal difference between the two \textbf{c.d.f.}'s;
    -   consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.\
3.  Calculate the largest possible value of $n$, for which the total number of decays in one second is less than $8 \times 10^8$ with probability at least $0.95$. To this end,
    -   obtain the theoretical bound on $n$ using Markov inequality, Chernoff bound and Central Limit Theorem, and compare the results;\
    -   simulate the realization $x_1,x_2,\dots,x_n$ of the $X_i$ and calculate the sum $s=x_1 + \cdots +x_n$;
    -   repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of sums;
    -   calculate the number of elements of the sample which are less than critical value ($8 \times 10^8$) and calculate the empirical probability; comment whether it is close to the desired level $0.95$

```{r}
T <- 30.1
m <- 35 * 10^(-6)
n <- 5
M <- 136.907089 # gram / mol this is the mass of c137
Na <- 6 * 10^23
n_list <- c(5, 10, 50)
# we need consistent units so our T must be converted from years to seconds we will calculate everything in seconds

T <- 30.1 * 365 * 24 * 60 * 60

T # = 949233600
# $\lambda$, is log(2)/T
# we found T so we can now calculate lambda (we need it to find mean)

lambda_for_all <- log(2) / T
lambda_for_all # = 7.302177e-10

# Now we can find N (number of atoms) = m/M NA

N <- m/M * Na
N # = 1.533887e+17

mu <- N * lambda_for_all
mu # = 112007142

# we have poisson distribution so it is easy for us to find sigma (= lambda) but we also should not forget that we search it per element and we have n of elements (=lambda/times). and sd would be sqrt of that value. BUT first I still calculated for all x-s 
var_for_all <- mu
#sd_for_all <- sqrt(var_for_all) # in other words it is for whole distribution but since they are independent than later on we can just devide it by n (sum of independent poisson rv = poisson rv with new parameter)
#sigma_for_all # = 1.460435e-10
#sd_for_all # = 1.208485e-05
K <- 1e3 # times that our experiment is going to be repeated 1000



sample_means <- colMeans(matrix(rpois(n*K, lambda = mu), nrow=n)) # functrion that calculates the mean value of each column in a matrix of df VERY QUICK
# rpois generates randm nums from poisson distribution first param - num of rvs second- mean and variance
```

#### Next, calculate the parameters of the standard normal approximation

```{r}
mu <- mu 
sigma <- sqrt(var_for_all/n)
```

#### We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     ylim = c(0,1),
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

**Next, proceed with all the remaining steps. To sum up:**

```{r}
max_diffs <- numeric(length(n_list))    
n_list <- c(5, 10, 50)
for (i in 1:length(n_list)) {
  n <- n_list[i]

  sample_means <- colMeans(matrix(rpois(n * K, lambda = mu), nrow = n))
  #Fs <- ecdf(sample_means)
  x_points <- seq(xlims[1], xlims[2], length.out = 1000)
  
  x_grid <- seq(min(sample_means), max(sample_means), length.out = 1000)
  ecdf_vals <- Fs(x_grid)
  cdf_vals  <- pnorm(x_grid, mean = mu, sd = sigma)
  
  max_diffs[i] <- max(abs(ecdf_vals - cdf_vals))
}

print("Maximal Differences")
names(max_diffs) <- paste("n=", n_list)
print(max_diffs)
print("As n gets bigger we can sees that diff between cdf and ecdf gets smaller and smaller. which numerically confirms clt")

```

```{r}
bound <- 8*10^8
# Markov's Inequality
# P(S n>= 8*10^8) <= E(Sn) / 8*10^8
# E(Sn) / 8*10^8 <= 1-0.95 = 0.05
# n * mu / 8*10^8 <= 0.05
# n <= (0.05 * 8*10^8) / mu
#print(bound)
#print(mu)
n_markov <- ((0.05 * bound) / mu)
cat("n <=", n_markov)


```

```{r}

# CLT
# We want P(Sn >=  8*10^8) <= 0.05
# 
# at first we can standardize it cause it has aproximately normal distribution (Sn - n*mu) / sqrt(n*mu) >= ( 8*10^8 - n*mu) / sqrt(n*mu)
# The 95th percentile for a standard Normal is z = 1.645
# So, we need: (8*10^8 - n*mu) / sqrt(n*mu) >= 1.645
# 8*10^8 - sqrt(n)^2*mu >= 1.645 * sqrt(n) * sqrt(mu)
# mu*sqrt(n)^2 + (1.645*sqrt(mu))*sqrt(n) -  8*10^8 <= 0

x <- (-( 1.645 * sqrt(mu)) + sqrt(( 1.645 * sqrt(mu))^2 - 4 * mu * (-bound))) / (2 * mu)
n_clt <- x^2

cat("CLT: n <=", x^2)

```

```{r}
#Chernoff
bound <- 8e8
mu_per_sample <- mu
desirable_prob <- 0.95

chernoff_log_root <- function(n) {
  lambda <- n * mu_per_sample # calculating parameter of  sum of poisson rv 
  log_prob <- -lambda + bound * (1 + log(lambda / bound)) 
  return(log_prob - log(1 - desirable_prob))
}

library(stats)
result <- uniroot(chernoff_log_root, lower = 1, upper = 8e8/mu)
n_chernoff <- result$root

cat("Chernoff bound n_max =", n_chernoff, "\n")

```

```{r}

K_large <- 10000

set.seed(35)
sample_sums <- rpois(K_large, lambda = n_clt * mu)

successes <- sum(sample_sums < 8e8)
empirical_prob <- successes / K_large

cat(" n =", n_clt, "with K =", K_large)
cat("  empirical probability of sum < 8*10^8 is:", empirical_prob, "\n")
print("The simulated probability is close to our target of 0.95, which confirms that our calculation for n using the CLT was correct")




```

Analyzing the plot: This plot is really beautiful, it clearly shows us that emperical cdf of our experimental data which is almost the same as theoretical cdf of Normal distribution with parameters that you can see above. First of all it is like visual confirmation that CLT works. Even though we had Poisson distribution for each particular x the distribution of the sample means is really close to normal distribution!!

What we have done in task 2:

1.  we had real-life problem where we needed to take a look at radioactive decay and then simpulate it and show that the sample means of actual numbers of decays
2.  we found average number of decays per second
3.  then we simulated the to find experimental averages ( we did it here: sample_means \<- ...)
4.  then we found theoretical cdf and compared it to ecdf

------------------------------------------------------------------------

Task 3.

In this task, we use the Central Limit Theorem approximation for continuous random variables.

One of the devices to measure radioactivity level at a given location is the Geiger counter. When the radioactive level is almost constant, the time between two consecutive clicks of the Geiger counter is an exponentially distributed random variable with parameter $\nu_1 = \mathtt{team\,id\,number} + 10$. Denote by $X_k$ the random time between the $(k-1)^{\mathrm{st}}$ and $k^{\mathrm{th}}$ click of the counter.

Show that the distribution of the sample means of $X_1, X_2,\dots,X_n$ gets very close to a normal one (which one?) as $n$ becomes large. To this end,

simulate the realizations $x_1,x_2,\dots,x_n$ of the $\textbf{r.v.}$ $X_i$ and calculate the sample mean $s=\overline{\mathbf{x}}$;

repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of means and then the $\textit{empirical cumulative distribution}$ function $F_{\mathbf{s}}$ of $\mathbf{s}$;

identify $\mu$ and $\sigma^2$ such that the $\textbf{c.d.f.}$ of $\mathscr{N}(\mu,\sigma^2)$ is close to the $\textbf{e.c.d.f.}$ $F_{\mathbf{s}}$ of and plot both $\textbf{c.d.f.}$'s on one graph to visualize their proximity;

calculate the maximal difference between the two $\textbf{c.d.f.}$'s;

consider cases $n = 5$, $n = 10$, $n=50$ and comment on the results.

The place can be considered safe when the number of clicks in one minute does not exceed $100$. It is known that the parameter $\nu$ of the resulting exponential distribution is proportional to the number $N$ of the radioactive samples, i.e., $\nu = \nu_1*N$, where $\nu_1$ is the parameter for one sample. Determine the maximal number of radioactive samples that can be stored in that place so that, with probability $0.95$, the place is identified as safe. To do this,

express the event of interest in terms of the $\textbf{r.v.}$ $S:= X_1 + \cdots + X_{100}$;

obtain the theoretical bounds on $N$ using the Markov inequality, Chernoff bound and Central Limit Theorem and compare the results;

with the predicted $N$ and thus $\nu$, simulate the realization $x_1,x_2,\dots,x_{100}$ of the $X_i$ and of the sum $S = X_1 + \cdots + X_{100}$;

repeat this $K$ times to get the sample $\mathbf{s}=(s_1,\dots,s_K)$ of total times until the $100^{\mathrm{th}}$ click;

estimate the probability that the location is identified as safe and compare to the desired level $0.95$

First, generate samples an sample means:

```{r}
nu1 <- 45
K <- 1e3
#n <- 5
n_list <- c(5, 10, 50)
for (n in n_list){
  sample_means <- colMeans(matrix(rexp(n*K, rate = nu1), nrow=n))
  cat("for n =", n, " mean of sample means = ", mean(sample_means))

}
```

Next, calculate the parameters of the standard normal approximation

```{r}
exp_mean <- 1/nu1
exp_var <- 1 / nu1^2
exp_sd <- exp_mean

mu <- exp_mean  
sigma <- exp_sd / sqrt(n)
#print(n)
```

We can now plot ecdf and cdf

```{r}
xlims <- c(mu-3*sigma,mu+3*sigma)
Fs <- ecdf(sample_means)
plot(Fs, 
     xlim = xlims, 
     col = "blue",
     lwd = 2,
     main = "Comparison of ecdf and cdf")
curve(pnorm(x, mean = mu, sd = sigma), col = "red", lwd = 2, add = TRUE)
```

```{r}
# now let's do it for 5, 10, 50 and find differences


n_list <- c(5, 10, 50)
max_diffs <- numeric(length(n_list))

for (i in seq_along(n_list)) {
  n <- n_list[i]
  mat <- matrix(rexp(n*K, rate = nu1), nrow = n, ncol = K)
  sample_means <- colMeans(mat)
  mu <- 1/nu1
  sigma <- (1/nu1) / sqrt(n)

  Fs <- ecdf(sample_means)
  xlim <- c(mu - 4*sigma, mu + 4*sigma)
  plot(Fs, xlim = xlim, col = "blue", lwd = 2,
       xlab = "mean", ylab = "CDF")
  curve(pnorm(x, mean = mu, sd = sigma), add = TRUE, col = "red", lwd = 2)

  x_grid <- seq(min(sample_means), max(sample_means), length.out = 1000)
  ecdf_vals <- Fs(x_grid)
  cdf_vals  <- pnorm(x_grid, mean = mu, sd = sigma)
  max_diffs[i] <- max(abs(ecdf_vals - cdf_vals))

  cat("n =", n, " mu =", round(mu,4), " sigma =", round(sigma,4),
      " max diff =", round(max_diffs[i],4), "\n")
}
# So here we measure how far the empirical distribution is from the theoretical normal distribution. We see that the bigger n we take, the smaller the difference between them becomes which signifies that the empirical distribution gets closer and closer to the normal distribution. This is a great example of Central Linit Theorem.
```

```{r}

#markov
nu1 <- 35 + 10    
m <- 100               
time_limit <- 60
desired_prob <- 0.95


# P(X <= 100) >= 0.95
# P(X >= 101) <= lambda/101 <= 0.05 ->
# -> lambda = 5,05
# and this is poisson distribution so we know our lambda and it is E(X) = nu1 * N *60 it is the radioactivity (nu1 * 60 sec) in N times

# nu1 * N *60 = 5.05
# we know nu1 = 45
# N = 5.05/45

lambda <- (1 - desired_prob) * (m + 1)
print(lambda)
N_markov <- lambda / (nu1)
print(N_markov)
#n_markov <- m / (3 * nu1)

# Markov's inequality gives us upper bound that will be true but will not be precise.
```

```{r}
#CLT 


a <- 1
b <- 1.645
c <- -100

x_solution <- (-b + sqrt(b^2 - 4*a*c)) / (2*a)
lambda_clt <- x_solution^2
N_clt_final <- lambda_clt / ( nu1)

cat("x =", x_solution, "\n")
cat("lambda =", lambda_clt, "\n")
cat("N_max, not rounded =", N_clt_final, "\n")
cat("N_max =", floor(N_clt_final), "\n\n")

# CLT gives us really good and precise estimation that is close to the true value.
```

```{r}
# Chernoff


mu <- 45
k <- 101
desired_prob <- 0.95

chernoff_log <- function(N) {
  exp(k - mu*N ) * (mu*N/k)^k - (1 - desired_prob)
}
N_chernoff <- uniroot(chernoff_log,  interval = c(1e-10, k/mu - 1e-6))$root
cat("Chernoff bound N_max =", N_chernoff, "\n")
cat("Rounded down =", floor(N_chernoff), "\n")

# Chernoff's inequality gives tighter upper bound than Markov's inequality but still not quite precise.

```

```{r}
nu1 <- 45
k <- 100
time_limit <- 60
desired_prob <- 0.95

cdf_diff <- function(lambda) {
  #we have k iid rv with exponential distribution with parameter x so we can say that thei sum will equal Gamma distribution with parameters 100 and x
  rate <- lambda / time_limit
  pgamma(time_limit, shape = k, scale = 1/rate) - (1-desired_prob)
}

sol <- uniroot(cdf_diff, lower = 1e-6, upper = 500)$root
lambda_true <- sol
N_true <- lambda_true / (60 * nu1)
cat("lambda_true =", lambda_true, " N_true =", N_true, "\n")

set.seed(123)
Ksim <- 20000
rate_sim <- lambda_true / time_limit
samps <- rgamma(Ksim, shape = k, scale = 1/rate_sim)
prob_est <- mean(samps > time_limit)
cat("P(s>60) emperical with lambda:", prob_est, "\n")

# This gives the true and numerically precise value that is also close to what CLT gave us earlier which again validates the results CLT gives us.
```

What we have done in task 3:

Part 1:

We observed and concluded that:

-   The empirical distributions of sample means $\overline{X}$ for exponential random variables converge rapidly to the normal distribution as the sample size $n$ increases.

-   The plots show that:

    -   For n=5, there is noticeable deviation from normality (distribution is still somewhat skewed).

    -   For n=10, the approximation is already quite good.

    -   For n=50, the ECDF and theoretical normal CDF are nearly identical — confirming the CLT very clearly.

Part 2:

-   We found the **maximum number of radioactive samples** that keeps the place safe (probability of at least 60 seconds between 100 clicks ≥ 0.95).

-   We used three theoretical methods — **Markov inequality**, **Chernoff bound**, and **Central Limit Theorem** — and compared them.

-   We also solved for the **exact Gamma case** numerically and verified the result by simulation.

-   The results showed that:

    -   Markov gives the lowest and most conservative N,
    -   Chernoff improves on it,
    -   CLT and simulation match almost perfectly.

### Task 4.

**This task consists of two parts:**

1.  **In this part, we discuss independence of random variables and its moments: expectation and variance.**

    1.  Suppose we have a random variable $X$. Explain why $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$;

    **Explanation**

    For the explanation let's use Jensen's inequality. It states that if $f$ is a convex function and $X$ is a random variable, then $f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)]$. For $f(x) = \frac{1}{x}$ (defined for $x>0$): $f'(x) = -\frac{1}{x^2}, \qquad f''(x) = \frac{2}{x^3} > 0$

    Hence, $f$ is convex on $(0, \infty)$.

    Applying Jensen’s inequality: $\frac{1}{\mathbb{E}[X]} = f(\mathbb{E}[X]) \leq \mathbb{E}[f(X)] = \mathbb{E}\!\left[\frac{1}{X}\right]$.

    Same applies to $x<0$ ($f′′(x)<0$ - function is concave) but the inequality will be reversed: $\frac{1}{\mathbb{E}[X]} \geq \mathbb{E}\!\left[\frac{1}{X}\right]$.

    2.  Let $X \sim \mathscr{N}(\mu,\sigma^2)$ with $\mu = teamidnumber$ and $\sigma^{2} = 2\times teamidnumber+7$. Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ and $y_1,y_2,\dots,y_{100}$ of $Y := \frac{1}{X}$ to calculate the values of $\frac{1}{\overline{\textbf{X}}}$ and $\overline{\textbf{Y}}$. Comment on the received results;

    ```{r}
    mu <- 35
    sigma <- 77
    N <- 100
    stadd <- sqrt(sigma)

    x <- rnorm(N, mean=mu, sd=stadd)
    mean_x <- mean(x)
    one_over_mean_x <- 1 / mean_x

    y <- 1 / x
    mean_y <- mean(y)

    cat("1/E(X) =", one_over_mean_x, "\n")
    cat("E(1/X) =", mean_y, "\n")
    # The output proves the inequality above.
    ```

    3.  Let $X$ and $Y$ be exponentially distributed r.v.'s with parameter $\lambda = 2$. Set $Z := \log{X} + 5$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Y$. Plot the Quantile-Quantile plot and scatterplot of $X$ and $Z$. Explain the results. Comment on the difference of relations between the pairs of random variables. Which pair of r.v.'s is dependent and which one is similar?

```{r}
X <- rexp(N, rate=2)
Y <- rexp(N, rate=2)
Z <- log(X) + 5

plot(X, Y, main="Scatterplot: X vs Y")
# There is a cloud-like image and no pattern on the plot. That proves that r. v. X and Y are independent.
qqplot(X, Y, main="Quantile-Quantile plot: X vs Y"); abline(0, 1, lty=2)
# Points are clustered near the line meaning that X and Y have similar distribution (some "rogue" points appear because of noise).
# So we conclude that X and Y are independent and similar.

plot(X, Z, main="Scatterplot: X vs Z")
# Points follow a strict pattern that reminds the graph of logarithm. And indeed Z is log(X)+5, exactly what we see on the plot. So, X and Z are dependent (Z is a function of X).
qqplot(X, Z, main="Quantile-Quantile plot: X vs Z"); abline(0, 1, lty=2)
# QQ-plot is also log-shaped signifying that the distribution of X and Z don't match (otherwise, we would see points lying on the diagonal as in case with X and Y). So X and Z are not similar.
# X and Z are dependent but not similar.
```

```         
------------------------------------------------------------------------
```

2.  You toss a fair coin three times and a random variable $X$ records how many times the coin shows Heads. You convince your friend that they should play a game with the following payoff: every round (equivalent to three coin tosses) will cost £$1$. They will receive £$0.5$ for every coin showing Heads. What is the expected value and the variance of the random variable $Y := 0.5X-1$?

    To answer this,

    1.  Explain what type of random variable is X:

        -   Normally distributed

        -   Binomially distributed

        -   Poisson distributed

        -   Uniformly distributed

        **Explanation**

        $X$ is binomially distributed value with parameters $n = 3$ (number of times we toss a coin) and $p = 0.5$ (probability to get a Head).

    2.  What are the expected value and variance of X? Simulate realizations $x_1,x_2,\dots,x_{100}$ of $X$ to calculate the values of sample mean $\overline{\mathbf{X}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(x_i - \overline{x})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    N <- 100
    X <- rbinom(N, size = 3, prob = 0.5)

    sample_mean_X <- mean(X)
    sample_var_X <- var(X)     # uses n-1 in denominator, had to look for R documentation for that

    # Let's also calculate theoretical mean and theoretical variance and compare them to the those we got via simulation
    theoretical_mean_X <- 3 * 0.5
    theoretical_var_X <- 3* 0.5 * (1- 0.5)

    cat("sample mean:", sample_mean_X, "\n")
    cat("sample variance:", sample_var_X, "\n")
    cat("theoretical mean:", theoretical_mean_X, "\n")
    cat("theoretical variance:", theoretical_var_X, "\n\n")
    print("We see that sample mean and sample variance are indeed close to their theoretical counterparts.")
    ```

    3.  What are the expected value and variance of Y? Simulate realizations $y_1,y_2,\dots,y_{100}$ of $Y$ to calculate the values of sample mean $\overline{\mathbf{Y}}$ and sample variance $s^2 = \frac{\sum_{i=1}^{n}{(y_i - \overline{y})^{2}}}{n-1}$. Comment on the results;

    ```{r}
    Y <- 0.5 * X - 1

    sample_mean_Y <- mean(Y)
    sample_var_Y <- var(Y)     # uses n-1 in denominator, had to look for R documentation for that

    # Again, let's calculate theoretical mean and theoretical variance and compare them to the those we got via simulation
    theoretical_mean_Y <- 0.5 * theoretical_mean_X - 1
    theoretical_var_Y <- (0.5)^2 * theoretical_var_X

    cat("sample mean:", sample_mean_Y, "\n")
    cat("sample variance:", sample_var_Y, "\n")
    cat("theoretical mean:", theoretical_mean_Y, "\n")
    cat("theoretical variance:", theoretical_var_Y, "\n\n")
    print("So again, we see that sample mean and sample variance are indeed close to their theoretical counterparts.In addition, after finding both theoretical and sample variance of Y, we see that it is not a goog idea to play this game, as on average, the player will lose money.")
    ```

What we have done in task 4:

Part 1:

-   We used Jensen's inequality to prove that $\mathbb{E}(\frac{1}{X}) \neq \frac{1}{\mathbb{E(X)}}$. This theoretical conclusion was further supported by practical proof via simulation.

-   We examined dependence and similarity between random variables. After plotting scatterplot and QQ-plot for X and Y we concluded that they were independent and similar. After doing the same for X and Z, we concluded that they were dependent but not similar.

Part 2:

-   We looked at the example of a game with coin. We concluded that r.v. $X\mathtt{\sim} Binomial(n=3, p=0.5)$

-   We found sample mean and variance for both X and Y. They both were close to theoretical mean and variance which demonstrated that the theory we learn at this course works in practice, and that was really nice to observe.

-   In addition, we concluded that the player of this game on average gets negative payoff, so it is better not to play it and avoid friends who want to get you to do so.

------------------------------------------------------------------------

### General summary and conclusions

Summarize here what you've done, whether you solved the tasks, what difficulties you had etc.
